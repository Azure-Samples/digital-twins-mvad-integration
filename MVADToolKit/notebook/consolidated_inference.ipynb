{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Up MVAD in Synapse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:39:21.3976883Z",
              "execution_start_time": "2022-02-01T23:39:21.3974608Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:35:35.8307082Z",
              "session_id": 59,
              "session_start_time": "2022-02-01T23:35:36.1199368Z",
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 59, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"name\": \"synapseml\",\n",
        "  \"conf\": {\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.5-19-82d6b563-SNAPSHOT\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,io.netty:netty-tcnative-boringssl-static\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import Necessary Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:39:35.2915134Z",
              "execution_start_time": "2022-02-01T23:39:30.024336Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:35:35.832561Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import synapse.ml\n",
        "import pyspark\n",
        "import re\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
        "from synapse.ml.cognitive import *\n",
        "from pyspark.sql import functions as F\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Parameter Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:39:35.5386425Z",
              "execution_start_time": "2022-02-01T23:39:35.3805511Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:35:35.8368901Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "start_time = None\n",
        "end_time = None\n",
        "\n",
        "scenario_name = None\n",
        "adls_container = None\n",
        "\n",
        "kusto_database = None\n",
        "adx_table = None\n",
        "\n",
        "adt_endpoint = None\n",
        "customer_adt_query = None\n",
        "relevant_metrics = None\n",
        "\n",
        "kv_name = None\n",
        "\n",
        "mvad_region = None\n",
        "\n",
        "# Newest param\n",
        "mvad_model_id = None\n",
        "resampling_rate = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "kv_linked_service = \"ADT_AnomalyDetector_KeyVault\"\n",
        "mvad_kv_secret_name = \"ad-poc\"\n",
        "adls_kv_connection_string_name = \"adls-connection-string\"\n",
        "kusto_linked_service = \"ADT_Data_History\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:39:35.8112705Z",
              "execution_start_time": "2022-02-01T23:39:35.8109777Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:35:35.8531529Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filter_metrics = \"' or Key == '\".join(relevant_metrics.split(\",\"))\n",
        "\n",
        "overall_query = \"evaluate azure_digital_twins_query_request('\" + adt_endpoint + \"', ```\" + customer_adt_query + \"```)\" + \\\n",
        "\" | extend Id = tostring(tid) | join kind=inner (\" + adx_table + \") on Id\" + \" | where Key == '\" + filter_metrics + \"'\" \n",
        "\n",
        "timerange = \"| where SourceTimeStamp between (datetime(\" + inference_start + \") .. datetime(\" + end_time + \"))\"\n",
        "new_query = overall_query + timerange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "anomalyKey = token_library.getSecret(kv_name, mvad_kv_secret_name, kv_linked_service)\n",
        "connectionString = token_library.getSecret(kv_name, adls_kv_connection_string_name, kv_linked_service)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check MVAD Model Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    # Request headers\n",
        "    'Ocp-Apim-Subscription-Key': anomalyKey,\n",
        "}\n",
        "params = urllib.parse.urlencode({})\n",
        "conn = http.client.HTTPSConnection(mvad_region + \".api.cognitive.microsoft.com\")\n",
        "conn.request(\"GET\", \"/anomalydetector/v1.1-preview.1/multivariate/models/\" + mvad_model_id + \"?%s\", \"{body}\", headers)\n",
        "response = conn.getresponse()\n",
        "data = response.read()\n",
        "conn.close()\n",
        "\n",
        "try:\n",
        "    model_status = json.loads(data)['modelInfo']['status']\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Model does not exist [HaltSubsequentInferenceRuns]\")\n",
        "\n",
        "if model_status == \"RUNNING\":\n",
        "    mssparkutils.notebook.exit(\"Success;Model training still in progress\")\n",
        "if model_status == \"FAILED\":\n",
        "    exit_message = \"Failure;\" + json.loads(data)['modelInfo']['errors'][0]['code']+\":\"+json.loads(data)['modelInfo']['errors'][0]['message'] + \" [HaltSubsequentInferenceRuns]\"\n",
        "    mssparkutils.notebook.exit(exit_message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Query and Data Preparation for MVAD Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:39:35.908187Z",
              "execution_start_time": "2022-02-01T23:39:35.9078566Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:35:35.8551524Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def query_adt_data(kusto_linked_service, kusto_database, query):\n",
        "    \"\"\" Query ADT data and return the data as Spark dataframe\n",
        "    :param kusto_linked_service: name of the ADX (historized data store) linked service registered in Synapse workspace\n",
        "    :type: string\n",
        "\n",
        "    :param kusto_database: ADX database name containing historized ADX data\n",
        "    :type: string\n",
        "\n",
        "    :param query: ADT-ADX joint query\n",
        "    :type: string\n",
        "\n",
        "    :return: dataframe containing queried data\n",
        "    :type: Spark dataframe\n",
        "    \"\"\"\n",
        "    df  = spark.read \\\n",
        "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "        .option(\"spark.synapse.linkedService\", kusto_linked_service) \\\n",
        "        .option(\"kustoDatabase\", kusto_database) \\\n",
        "        .option(\"kustoQuery\", query) \\\n",
        "        .option(\"authType\", \"LS\") \\\n",
        "        .load()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def preprocess(purpose=None, \\\n",
        "               raw_df=None, \\\n",
        "               invalid_values=['None', 'NaN', 'NA', 'nan', '', ' ', -1], \\\n",
        "               resampling_rate='1min', \\\n",
        "               num_agg_fc='mean', \\\n",
        "               cat_agg_fc='mode', \\\n",
        "               missing_tolerance=1.0, \\\n",
        "               limit=None, \\\n",
        "               num_range_dic_train=None, \\\n",
        "               num_uniqueKeys_train=None, \\\n",
        "               cat_uniqueKeys_values_dic_train=None, \\\n",
        "               values_to_fill_for_inference=None) -> tuple or pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Implement data pre-processing for raw data from ADT-ADX cross query in training pipeline, according to the guidelines indicated in PR FAQ.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    purpose : indicate the purpose of data pre-processing, one of 'training' or 'inference',\n",
        "        str\n",
        "    raw_df : raw training or inference data from ADX, with columns 'Id', 'ModelId', 'Key', 'Timestamp' and 'Value',\n",
        "        Spark DataFrame\n",
        "    invalid_values : list of entries that considered as invalid,\n",
        "        list, default=['None', 'NaN', 'NA', 'nan', '', ' ', -1]\n",
        "    resampling_rate : resampling rate of timestamp,\n",
        "        str, default='1min'\n",
        "    num_agg_fc : numerical variables aggregation function when resampling,\n",
        "        str or function, default='mean'\n",
        "    cat_agg_fc : categorical variables aggregation function when resampling,\n",
        "        str or function, default='mode'\n",
        "    missing_tolerance : tolerance of missing ratio for each variable, only columns with missing ratio not exceeding this threshold will be kept,\n",
        "        float, default=1.0\n",
        "    limit : max number of consecutive missings to fill,\n",
        "        int\n",
        "    num_range_dic_train, num_uniqueKeys_train, cat_uniqueKeys_values_dic_train, values_to_fill_for_inference : params only passed to inference which obtained from training,\n",
        "        for details see Return below.\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    ret_df : pre-processed training or inference data,\n",
        "        Spark DataFrame\n",
        "    num_range_dic_train : dict of all numerical features' min and max summarized from the training dataset,\n",
        "        dict (e.g. {'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ia': {'min': 0.0, 'max': 100},\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ib': {'min': 0.0, 'max': 200},\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ic': {'min': 0.0, 'max': 300})\n",
        "    num_uniqueKeys : list of numerical unique keys seen in the training dataset, each unique keys is formatted as 'ModelId_Id_Key',\n",
        "        list of str (e.g. ['dtmi:syntheticfactory:feedmachine;1_C_Amps_Ia', \n",
        "                           'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ib', \n",
        "                           'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ic'])\n",
        "    cat_uniqueKeys_values_dic : dict of all values seen in the training dataset for each categorical unique key,\n",
        "        dict (e.g. {'dtmi:syntheticfactory:sourcemachine;1_A_PowerLevel': ['High', 'Low', 'Mid'],\n",
        "                    'dtmi:syntheticfactory:sourcemachine;1_B_PowerLevel': ['High', 'Low', 'Mid']}})\n",
        "    values_to_fill_for_inference : dict of default value to fill in during inference for each unique key,\n",
        "        dict (e.g. {'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ia': 0.0,\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ib': 0.0,\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ic': 0.0})\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Example input data for data pre-processing:\n",
        "    index   Id                   ModelId                      Key                  Timestamp                      Value\n",
        "    0       E     dtmi:syntheticfactory:feedmachine;1       Amps_Ic        2020-12-31 23:59:59.028164              0.0\n",
        "    1       J     dtmi:syntheticfactory:feedmachine;1       Amps_Ib        2020-12-31 23:59:59.273131     0.0059823441449033355\n",
        "    2       A     dtmi:syntheticfactory:sourcemachine;1     Amps_Ia        2020-12-31 23:59:59.285524     0.002924511047766594\n",
        "    3       A     dtmi:syntheticfactory:sourcemachine;1    PowerLevel      2021-01-01 00:00:00.840780              Mid\n",
        "    4       B     dtmi:syntheticfactory:sourcemachine;1    PowerLevel      2021-01-01 00:00:01.250716              Low\n",
        "    ...    ...                     ...                        ...                     ...                          ...\n",
        "\n",
        "    Example output of pre-processed data:\n",
        "    index       timestamp        dtmi:syntheticfactory:sourcemachine;1_A_Amps_Ia    dtmi:syntheticfactory:sourcemachine;1_A_PowerLevel_High     ...\n",
        "    0      2021-01-01 00:00:00                          0                                                      0                                ...\n",
        "    1      2021-01-01 00:10:00                          0                                                      1                                ...\n",
        "    2      2021-01-01 00:20:00                       0.000114                                                  1                                ...\n",
        "    3      2021-01-01 00:30:00                       0.000097                                                  0                                ...\n",
        "    4      2021-01-01 00:40:00                          0                                                      0                                ...\n",
        "    \"\"\"\n",
        "    ## Step 1. Query & Basic Data Quality Checks\n",
        "    # Step 1.1. Convert Spark to Pandas df, reformat the raw data with columns: 'timestamp', 'UniqueKey', 'value'\n",
        "    print('Data Shape before pre-processing:', (raw_df.count(), len(raw_df.columns)))\n",
        "    if (raw_df.count() == 0):\n",
        "        print('Empty Dataset to pre-process.')\n",
        "        return\n",
        "    \n",
        "    raw_df = raw_df.drop('TimeStamp')\n",
        "    raw_df = raw_df.withColumnRenamed('SourceTimeStamp', 'TimeStamp')\n",
        "\n",
        "    # Format timestamp to MVAD accepted format\n",
        "    raw_df = raw_df.withColumn(\n",
        "        'TimeStamp', \n",
        "        F.date_format(F.col('TimeStamp'), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
        "        )\n",
        "    \n",
        "    # Change non alphanumeric or _ characters to _ for ModelId and Id columns\n",
        "    raw_df = raw_df.withColumn(\"ModelId\", F.regexp_replace(F.col(\"ModelId\"), \"[^a-zA-Z0-9_]\", \"_\"))\n",
        "    raw_df = raw_df.withColumn(\"Id\", F.regexp_replace(F.col(\"Id\"), \"[^a-zA-Z0-9_]\", \"_\"))\n",
        "\n",
        "    # Create UniqueKey column to identify unique key per ModelId, Id, Key combinations\n",
        "    raw_df = raw_df.withColumn('UniqueKey', \n",
        "                    F.concat(F.col('ModelId'), F.lit('_'), F.col('Id'), F.lit('_'), F.col('Key')))\n",
        "\n",
        "    # Select needed columns and make column names lowercase\n",
        "    raw_df = raw_df.select(\"TimeStamp\", \"UniqueKey\", \"Value\").withColumnRenamed(\"TimeStamp\", \"timestamp\").withColumnRenamed(\"Value\", \"value\")\n",
        "\n",
        "    # Convert Spark to Pandas df\n",
        "    raw_df = raw_df.toPandas()\n",
        "    \n",
        "    raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'], infer_datetime_format=True) \n",
        "    raw_df = raw_df[['timestamp', 'UniqueKey', 'value']] \\\n",
        "             .sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "    # Step 1.2. Drop duplicate rows in raw historized dataset\n",
        "    df = raw_df.drop_duplicates()\n",
        "    # Step 1.3. Data validity checks\n",
        "    df = df[~df['value'].isin(invalid_values)].reset_index(drop=True)\n",
        "\n",
        "    ## Step 2. Resampling (timestamp alignment) & Pivoting\n",
        "    # Step 2.1. Timestamp binning\n",
        "    df['timestamp'] = df['timestamp'].dt.round(resampling_rate)\n",
        "    # Step 2.2. Table pivoting\n",
        "    tmp_initial_twins = df.groupby('UniqueKey')['value'].apply(lambda x: list(x)[0]).reset_index()\n",
        "    tmp_initial_twins_dic = dict(zip(tmp_initial_twins['UniqueKey'], tmp_initial_twins['value']))\n",
        "    if purpose=='training':\n",
        "        num_uniqueKeys, cat_uniqueKeys = [], []\n",
        "        for k, v in tmp_initial_twins_dic.items():\n",
        "            try:\n",
        "                float(v)\n",
        "                num_uniqueKeys.append(k)\n",
        "            except:\n",
        "                cat_uniqueKeys.append(k)\n",
        "        num_uniqueKeys, cat_uniqueKeys = sorted(num_uniqueKeys), \\\n",
        "                                         sorted(cat_uniqueKeys)\n",
        "    else:\n",
        "        num_uniqueKeys_inf, cat_uniqueKeys_inf = [], []\n",
        "        for k, v in tmp_initial_twins_dic.items():\n",
        "            try:\n",
        "                float(v)\n",
        "                num_uniqueKeys_inf.append(k)\n",
        "            except:\n",
        "                cat_uniqueKeys_inf.append(k)\n",
        "        num_uniqueKeys_inf, cat_uniqueKeys_inf = sorted(num_uniqueKeys_inf), \\\n",
        "                                                 sorted(cat_uniqueKeys_inf)\n",
        "        num_uniqueKeys_unknown, num_uniqueKeys_missing = [col for col in num_uniqueKeys_inf if col not in num_uniqueKeys_train], \\\n",
        "                                                        [col for col in num_uniqueKeys_train if col not in num_uniqueKeys_inf]\n",
        "        cat_uniqueKeys_train = sorted(list(cat_uniqueKeys_values_dic_train.keys()))\n",
        "        cat_uniqueKeys_unknown, cat_uniqueKeys_missing = [col for col in cat_uniqueKeys_inf if col not in cat_uniqueKeys_train], \\\n",
        "                                                        [col for col in cat_uniqueKeys_train if col not in cat_uniqueKeys_inf]\n",
        "        num_uniqueKeys, cat_uniqueKeys = [col for col in num_uniqueKeys_inf if col in num_uniqueKeys_train], \\\n",
        "                                         [col for col in cat_uniqueKeys_inf if col in cat_uniqueKeys_train]\n",
        "\n",
        "    num_df, cat_df = df[df['UniqueKey'].isin(num_uniqueKeys)], \\\n",
        "                     df[df['UniqueKey'].isin(cat_uniqueKeys)]\n",
        "    num_df['value'] = num_df['value'].astype(float)\n",
        "\n",
        "    num_df_after_groupby = num_df.groupby(['timestamp', 'UniqueKey'])['value'].mean().reset_index() if num_agg_fc=='mean' \\\n",
        "                           else num_df.groupby(['timestamp', 'UniqueKey'])['value'].apply(num_agg_fc).reset_index()\n",
        "    cat_df_after_groupby = cat_df.groupby(['timestamp', 'UniqueKey'])['value'].agg(lambda x: pd.Series.mode(x)[0]).reset_index() if cat_agg_fc=='mode' \\\n",
        "                           else cat_df.groupby(['timestamp', 'UniqueKey'])['value'].apply(cat_agg_fc).reset_index()\n",
        "    df_after_groupby = pd.concat([num_df_after_groupby, cat_df_after_groupby])\n",
        "    df_after_groupby = df_after_groupby.sort_values('timestamp').reset_index(drop=True)\n",
        "    df_pivot = df_after_groupby.pivot(index='timestamp', columns='UniqueKey', values='value')\n",
        "    df_pivot.columns.name = None\n",
        "    df_pivot = df_pivot.sort_index().reset_index()\n",
        "\n",
        "    if purpose=='inference':\n",
        "        print(f'Inference - Unknown Numerical Keys and Dropped: {num_uniqueKeys_unknown}')\n",
        "        print(f'Inference - Unknown Categorical Keys and Dropped: {cat_uniqueKeys_unknown}')\n",
        "        print(f'Inference - Missing Numerical Keys: {num_uniqueKeys_missing}, Filled with {[values_to_fill_for_inference[col] for col in num_uniqueKeys_missing]}')\n",
        "        print(f'Inference - Missing Categorical Keys: {cat_uniqueKeys_missing}, Filled with {[values_to_fill_for_inference[col] for col in cat_uniqueKeys_missing]}')\n",
        "        for col in num_uniqueKeys_missing:\n",
        "            df_pivot[col] = values_to_fill_for_inference[col]\n",
        "            df_pivot[col] = df_pivot[col].astype(float)\n",
        "        for col in cat_uniqueKeys_missing:\n",
        "            df_pivot[col] = values_to_fill_for_inference[col]\n",
        "        df_pivot = df_pivot[sorted(df_pivot.columns)]\n",
        "        \n",
        "        cat_uniqueKeys_values_dic_inf = {}\n",
        "        cat_uniqueKeys_values_unknown_dic, cat_uniqueKeys_values_missing_dic = {}, {}\n",
        "        for col in cat_uniqueKeys_train:\n",
        "            cat_uniqueKeys_values_dic_inf[col] = sorted(df_pivot[col][df_pivot[col].notnull()].unique())\n",
        "            cat_uniqueKeys_values_unknown_dic[col] = [v for v in cat_uniqueKeys_values_dic_inf[col] if v not in cat_uniqueKeys_values_dic_train[col]]\n",
        "            cat_uniqueKeys_values_missing_dic[col] = [v for v in cat_uniqueKeys_values_dic_train[col] if v not in cat_uniqueKeys_values_dic_inf[col]]\n",
        "            if cat_uniqueKeys_values_unknown_dic[col]!=[]:\n",
        "                print(f\"\\nInference - Unknown Categorical Values for '{col}' and Ignored: {cat_uniqueKeys_values_unknown_dic[col]}\")\n",
        "                df_pivot[col] = df_pivot[col].replace(cat_uniqueKeys_values_unknown_dic[col], np.nan)\n",
        "\n",
        "    # Step 2.3. Timestamp standardization\n",
        "    missing_timestamp_df = pd.DataFrame(columns=df_pivot.columns)\n",
        "    if df_pivot.shape[0]>1:   \n",
        "        full_time_range = pd.date_range(df_pivot['timestamp'].iloc[0], \\\n",
        "                                        df_pivot['timestamp'].iloc[-1], \\\n",
        "                                        freq=resampling_rate)\n",
        "        full_time_range.freq=None\n",
        "        missing_timestamp_df['timestamp'] = [timestamp for timestamp in full_time_range \\\n",
        "                                            if timestamp not in df_pivot['timestamp'].values]\n",
        "        df_pivot = pd.concat([df_pivot, missing_timestamp_df])\n",
        "    df_pivot = df_pivot.sort_values('timestamp').set_index('timestamp')\n",
        "    # Step 2.4. Calculate missing ratios for each feature\n",
        "    if purpose=='training':\n",
        "        missing_ratios = (df_pivot.isnull().sum()/df_pivot.shape[0]).sort_values(ascending=False)\n",
        "        df_pivot = df_pivot[missing_ratios[missing_ratios<=missing_tolerance].index]\n",
        "        df_pivot = df_pivot[sorted(df_pivot.columns)]\n",
        "\n",
        "        num_uniqueKeys, cat_uniqueKeys = sorted([col for col in num_uniqueKeys if col in df_pivot.columns]), \\\n",
        "                                         sorted([col for col in cat_uniqueKeys if col in df_pivot.columns])\n",
        "        values_to_fill_for_inference = {}\n",
        "        for col in num_uniqueKeys:\n",
        "            values_to_fill_for_inference[col] = df_pivot[col].median()\n",
        "        for col in cat_uniqueKeys:\n",
        "            values_to_fill_for_inference[col] = df_pivot[col].mode()[0]\n",
        "        \n",
        "        cat_uniqueKeys_values_dic = {}\n",
        "        for cat_uniqueKey in cat_uniqueKeys:\n",
        "            cat_uniqueKeys_values_dic[cat_uniqueKey] = sorted(df_pivot[cat_uniqueKey][df_pivot[cat_uniqueKey].notnull()].unique())\n",
        "\n",
        "    ## Step 3. Data-preprocessing on pivoted and standardized table\n",
        "    # Step 3.1. Handling missing NaN values\n",
        "    for col in df_pivot:\n",
        "        if col in num_uniqueKeys:\n",
        "            df_pivot[col] = df_pivot[col].astype(float).interpolate(method='linear', limit=limit)\n",
        "        df_pivot[col] = df_pivot[col].fillna(method='ffill', limit=limit).fillna(method='bfill', limit=limit)\n",
        "    assert df_pivot.isnull().sum().sum()==0\n",
        "    # Step 3.2. Normalization & Encoding\n",
        "    num_df_pivot, cat_df_pivot = df_pivot.select_dtypes(include='float'), \\\n",
        "                                 df_pivot.select_dtypes(include='object')\n",
        "\n",
        "    if purpose=='training':\n",
        "        num_range_dic_train, num_min_dic, num_max_dic = dict(), \\\n",
        "                                                  dict(num_df_pivot.min()), \\\n",
        "                                                  dict(num_df_pivot.max())\n",
        "        for k in num_min_dic:\n",
        "            num_range_dic_train[k] = dict()\n",
        "            num_range_dic_train[k]['min'], num_range_dic_train[k]['max'] = num_min_dic[k], \\\n",
        "                                                               num_max_dic[k]\n",
        "    for col in num_df_pivot.columns:\n",
        "        if num_range_dic_train[col]['max']==num_range_dic_train[col]['min']:\n",
        "            num_df_pivot[col] = num_df_pivot[col]-num_range_dic_train[col]['min']\n",
        "        else:\n",
        "            num_df_pivot[col] = (num_df_pivot[col]-num_range_dic_train[col]['min'])/(num_range_dic_train[col]['max']-num_range_dic_train[col]['min']) \n",
        "    num_df_pivot = num_df_pivot.reset_index()\n",
        "    \n",
        "    if not cat_df_pivot.empty:\n",
        "        cat_df_pivot = pd.get_dummies(cat_df_pivot)\n",
        "    else:\n",
        "        cat_df_pivot = pd.DataFrame(index=cat_df_pivot.index)\n",
        "    cat_df_pivot = cat_df_pivot.reset_index()\n",
        "    if purpose=='inference':\n",
        "        for col in cat_uniqueKeys_train:\n",
        "            if cat_uniqueKeys_values_missing_dic[col]!=[]:\n",
        "                print(f\"Inference - Missing Categorical Values for '{col}', Completed with: {cat_uniqueKeys_values_missing_dic[col]}\")\n",
        "                for missing_value in cat_uniqueKeys_values_missing_dic[col]:\n",
        "                    new_dummy_col_name = col + '_' + missing_value\n",
        "                    cat_df_pivot[new_dummy_col_name] = 0\n",
        "    ret_df = num_df_pivot.merge(cat_df_pivot, on='timestamp', how='inner')\n",
        "    ret_df = ret_df.sort_values('timestamp').reset_index(drop=True)\n",
        "    ret_df = ret_df[sorted(ret_df.columns)]\n",
        "    print(f'Data Shape after pre-processing: {ret_df.shape}')\n",
        "    print(ret_df.head())\n",
        "\n",
        "    ret_df['timestamp'] = ret_df['timestamp'].apply(lambda x: x.isoformat())\n",
        "\n",
        "    # Convert dataframe to Spark\n",
        "    ret_df = spark.createDataFrame(ret_df)\n",
        "    ret_df = ret_df.withColumn('timestamp', F.concat_ws(\"\",F.col(\"timestamp\"),F.lit(\"Z\")))\n",
        "    \n",
        "    if purpose=='training':\n",
        "        return ret_df, num_range_dic_train, num_uniqueKeys, cat_uniqueKeys_values_dic, values_to_fill_for_inference\n",
        "    else:\n",
        "        return ret_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:40:31.1317683Z",
              "execution_start_time": "2022-02-01T23:39:36.3254391Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:35:35.8638542Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 9
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 9, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    df = query_adt_data(kusto_linked_service, kusto_database, new_query)\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Unable to query data from ADX. Full error log: \" + str(e))\n",
        "\n",
        "if (df.count() == 0):\n",
        "    mssparkutils.notebook.exit(\"Failure;No Data Queried from ADX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Check if training data preprocessing configurations are passed in as pipeline parameter\n",
        "    if training_args is not None:\n",
        "        train_args = ast.literal_eval(training_args)\n",
        "    # Check if training data preprocessing configurations in Kusto if it's not passed in as pipeline parameter\n",
        "    else:\n",
        "        train_args = query_adt_data(kusto_linked_service, kusto_database, 'metadataTable | take 1 | where scenarioName == \"' + scenario_name + '\" | project additionalNote')\n",
        "        train_args = ast.literal_eval(train_args.select('additionalNote').collect()[0][0])\n",
        "except Exception:\n",
        "    mssparkutils.notebook.exit(\"Failure;No training data preprocessing configurations\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    to_save_df = preprocess(purpose='inference',\n",
        "                    raw_df=df, \\\n",
        "                    resampling_rate=train_args['resampling_rate'], \\\n",
        "                    num_agg_fc=train_args['num_agg_fc'], \\\n",
        "                    cat_agg_fc=train_args['cat_agg_fc'], \\\n",
        "                    num_range_dic_train=train_args[\"num_range_dic_train\"], \\\n",
        "                    num_uniqueKeys_train=train_args[\"num_uniqueKeys_train\"], \\\n",
        "                    cat_uniqueKeys_values_dic_train=train_args[\"cat_uniqueKeys_values_dic_train\"], \\\n",
        "                    values_to_fill_for_inference=train_args[\"values_to_fill_for_inference\"])\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Data processing failure. Full error log: \" + str(e))\n",
        "\n",
        "if (to_save_df.count() == 0):\n",
        "    mssparkutils.notebook.exit(\"Failure;No Data After Data Processing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# MVAD Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:40:33.5224217Z",
              "execution_start_time": "2022-02-01T23:40:33.3666941Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:35:42.3764987Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 12
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 12, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Specify information about your data.\n",
        "timestampColumn = \"timestamp\" \n",
        "# inputColumns = train_args[\"num_uniqueKeys_train\"] + list(train_args[\"cat_uniqueKeys_values_dic_train\"].keys())\n",
        "inputColumns = to_save_df.columns\n",
        "inputColumns.remove(\"timestamp\")\n",
        "\n",
        "#Specify the container you created in Storage account. \n",
        "containerName = adls_container\n",
        "\n",
        "#Set a folder name in Storage account to store the intermediate data. \n",
        "intermediateSaveDir = \"mvad_temporary_files\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:55:13.6766644Z",
              "execution_start_time": "2022-02-01T23:55:13.5335582Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:55:13.1334185Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 24
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 24, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    retrievedModel = (DetectMultivariateAnomaly()\n",
        "        .setSubscriptionKey(anomalyKey)\n",
        "        .setLocation(mvad_region)\n",
        "        .setStartTime(start_time)\n",
        "        .setEndTime(end_time)\n",
        "        .setContainerName(containerName)\n",
        "        .setIntermediateSaveDir(intermediateSaveDir)\n",
        "        .setInputCols(inputColumns)\n",
        "        .setConnectionString(connectionString)\n",
        "        .setModelId(mvad_model_id))\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Unable to create DetectMultivariateAnomaly object. Full error log: \" + str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "startInferenceTime = inference_start\n",
        "endInferenceTime = end_time\n",
        "\n",
        "try:\n",
        "      result = (retrievedModel\n",
        "            .setStartTime(startInferenceTime)\n",
        "            .setEndTime(endInferenceTime)\n",
        "            .setOutputCol(\"result\")\n",
        "            .setTimestampCol(timestampColumn)\n",
        "            .setInputCols(inputColumns)\n",
        "            .transform(to_save_df))\n",
        "      display(result)\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Inference Failure. Full error log: \" + str(e))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:55:32.0159987Z",
              "execution_start_time": "2022-02-01T23:55:31.8717138Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T23:55:19.870279Z",
              "session_id": 59,
              "session_start_time": null,
              "spark_pool": "newSpark3",
              "state": "finished",
              "statement_id": 26
            },
            "text/plain": [
              "StatementMeta(newSpark3, 59, 26, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "retrievedModel.cleanUpIntermediateData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "result = result.filter(result.timestamp > start_time)\n",
        "\n",
        "# convert timestamp column from string to timestamp datatype\n",
        "result = result.withColumn(\n",
        "        'timestamp', \n",
        "        F.date_format(F.col('timestamp'), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
        "        )\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Write Result to ADX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write to ADX\n",
        "\n",
        "try:\n",
        "        sp = sc._jvm.com.microsoft.kusto.spark.datasink.SparkIngestionProperties(\n",
        "                True, [\"dropByTags\"], [\"ingestByTags\"], [\"tags\"], [\"ingestIfNotExistsTags\"], None, None, None)\n",
        "        result.write \\\n",
        "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "        .option(\"spark.synapse.linkedService\", kusto_linked_service) \\\n",
        "        .option(\"kustoDatabase\", kusto_database) \\\n",
        "        .option(\"kustoTable\", scenario_name) \\\n",
        "        .option(\"sparkIngestionPropertiesJson\", sp.toString()) \\\n",
        "        .option(\"tableCreateOptions\",\"CreateIfNotExist\") \\\n",
        "        .mode(\"Append\") \\\n",
        "        .save()\n",
        "except Exception as e:\n",
        "        mssparkutils.notebook.exit(\"Failure;Unable to Write Inference Result to ADX. Full error log: \" + str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T23:40:33.7658613Z",
              "execution_start_time": null,
              "livy_statement_state": null,
              "queued_time": "2022-02-01T23:35:45.8213906Z",
              "session_id": null,
              "session_start_time": null,
              "spark_pool": null,
              "state": "cancelled",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Cancelled, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mssparkutils.notebook.exit(\"Success;Successful Inference\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
