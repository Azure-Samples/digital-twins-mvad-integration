{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set Up MVAD in Synapse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"name\": \"synapseml\",\n",
        "  \"conf\": {\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.5-19-82d6b563-SNAPSHOT\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,io.netty:netty-tcnative-boringssl-static\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Necessary Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import synapse.ml\n",
        "import pyspark\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from synapse.ml.cognitive import *\n",
        "from datetime import datetime\n",
        "from pyspark.sql import functions as F\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from notebookutils import mssparkutils\n",
        "from sklearn.ensemble import IsolationForest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set Parameter Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "start_time = None\n",
        "end_time = None\n",
        "\n",
        "scenario_name = None\n",
        "adls_container = None\n",
        "\n",
        "kusto_database = None\n",
        "adx_table = None\n",
        "\n",
        "adt_endpoint = None\n",
        "customer_adt_query = None\n",
        "relevant_metrics = None\n",
        "\n",
        "kv_name = None\n",
        "\n",
        "mvad_region = None\n",
        "\n",
        "sliding_window = None\n",
        "resampling_rate = None\n",
        "smoothing = None\n",
        "\n",
        "adx_mapping_Id = None\n",
        "adx_mapping_key = None\n",
        "adx_mapping_value = None\n",
        "adx_mapping_sourcetime = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kv_linked_service = \"ADT_AnomalyDetector_KeyVault\"\n",
        "mvad_kv_secret_name = \"ad-poc\"\n",
        "adls_kv_connection_string_name = \"adls-connection-string\"\n",
        "kusto_linked_service = \"ADT_Data_History\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# strip inputs\n",
        "start_time = start_time.strip()\n",
        "end_time = end_time.strip()\n",
        "\n",
        "scenario_name = scenario_name.strip()\n",
        "adls_container = adls_container.strip()\n",
        "\n",
        "kusto_database = kusto_database.strip()\n",
        "adx_table = adx_table.strip()\n",
        "\n",
        "adt_endpoint = adt_endpoint.strip()\n",
        "customer_adt_query = customer_adt_query.strip()\n",
        "relevant_metrics = relevant_metrics.strip()\n",
        "\n",
        "kv_name = kv_name.strip()\n",
        "\n",
        "mvad_region = mvad_region.strip()\n",
        "\n",
        "adx_mapping_Id = adx_mapping_Id.strip()\n",
        "adx_mapping_key = adx_mapping_key.strip()\n",
        "adx_mapping_value = adx_mapping_value.strip()\n",
        "adx_mapping_sourcetime = adx_mapping_sourcetime.strip()\n",
        "\n",
        "# check empty scenario_name\n",
        "if not scenario_name:\n",
        "    mssparkutils.notebook.exit(\"Failure;: 'scenario_name' can not be empty.\")\n",
        "\n",
        "# check datetime format\n",
        "try:\n",
        "    datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n",
        "    datetime.fromisoformat(end_time.replace('Z', '+00:00'))\n",
        "except:\n",
        "    mssparkutils.notebook.exit(\"Failure;: check your 'start_time' and 'end_time' to be ISO-8601 format.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filter_metrics = (\"' or \" + adx_mapping_key + \" == '\").join(relevant_metrics.split(\",\"))\n",
        "\n",
        "overall_query = \"evaluate azure_digital_twins_query_request('\" + adt_endpoint + \"', ```\" + customer_adt_query + \"```)\" + \\\n",
        "\" | extend \" + adx_mapping_Id + \" = tostring(tid) | join kind=inner (\" + adx_table + \") on \" + adx_mapping_Id + \" | where \" + adx_mapping_key + \" == '\" + filter_metrics + \"'\" \n",
        "\n",
        "timerange = \"| where SourceTimeStamp between (datetime(\" + start_time + \") .. datetime(\" + end_time + \"))\"\n",
        "new_query = overall_query + timerange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Query and Data Preparation for MVAD Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_adt_data(kusto_linked_service, kusto_database, query):\n",
        "    \"\"\" Query ADT data and return the data as Spark dataframe\n",
        "    :param kusto_linked_service: name of the ADX (historized data store) linked service registered in Synapse workspace\n",
        "    :type: string\n",
        "\n",
        "    :param kusto_database: ADX database name containing historized ADX data\n",
        "    :type: string\n",
        "\n",
        "    :param query: ADT-ADX joint query\n",
        "    :type: string\n",
        "\n",
        "    :return: dataframe containing queried data\n",
        "    :type: Spark dataframe\n",
        "    \"\"\"\n",
        "    df  = spark.read \\\n",
        "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "        .option(\"spark.synapse.linkedService\", kusto_linked_service) \\\n",
        "        .option(\"kustoDatabase\", kusto_database) \\\n",
        "        .option(\"kustoQuery\", query) \\\n",
        "        .option(\"authType\", \"LS\") \\\n",
        "        .load()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smoothing(df=None, preprocessed=False, clipping=True, univariate_ad=True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Smooth data for training, either it was pre-processed beforehand or not, via replacing outliers by clipping and/or uni-variate anomaly detection.\n",
        "    1. For clipping method, lower and higher outliers will be replaced by mu-3*std and mu+3*std, respectively.\n",
        "    2. For uni-variate anomaly detection (IsolationForest), detected anomalies (outliers) will be replaced by the last previous normal value,\n",
        "       or the first next normal value if the former is not available.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : dataframe to smooth, with or without data-preprocessing before,\n",
        "        pd.DataFrame\n",
        "    preprocessed : indicate if 'df' was preprocessed beforehand,\n",
        "        bool, default=False\n",
        "    clipping : indicate whether to smooth 'df' by clipping outliers for each time-series \n",
        "        (outliers are defined as deviating the mean by more than 3 standard deviation by default),\n",
        "        bool, default=True\n",
        "    univariate_ad : indicate whether to smooth 'df' by filtering out outliers obtained from \n",
        "        uni-variate anomaly detection algorithm (IsolationForest) for each time-series,\n",
        "        bool, default=True\n",
        "    \n",
        "    Return\n",
        "    ----------\n",
        "    df : Smoothed data after clipping and/or uni-variate anomaly detection application,\n",
        "        pd.DataFrame\n",
        "    \"\"\"\n",
        "    if preprocessed:\n",
        "        if clipping:\n",
        "            df_clip = pd.DataFrame()\n",
        "            for col in df.columns:\n",
        "                if col != 'timestamp':\n",
        "                    tmp_df_clip = df[col]\n",
        "                    mu, std = tmp_df_clip.mean(), tmp_df_clip.std()\n",
        "                    df_clip = pd.concat([df_clip, tmp_df_clip.clip(mu-3*std, mu+3*std)], axis=1)\n",
        "                else:\n",
        "                    df_clip = pd.concat([df_clip, df[col]], axis=1)\n",
        "        \n",
        "        if univariate_ad:\n",
        "            df_to_univariate_ad = df_clip.copy() if clipping else df.copy()\n",
        "            df_univariate_ad = pd.DataFrame()\n",
        "            for col in df_to_univariate_ad.columns:\n",
        "                if col!= 'timestamp':\n",
        "                    iso_forest = IsolationForest(n_estimators=125)\n",
        "                    tmp_df_univariate_ad = pd.DataFrame(df_to_univariate_ad[col].copy())\n",
        "                    tmp_df_univariate_ad['predictions'] = iso_forest.fit_predict(df_to_univariate_ad[col].to_numpy().reshape(-1,1))\n",
        "                    tmp_df_univariate_ad.loc[tmp_df_univariate_ad['predictions']==-1, col] = np.nan\n",
        "                    tmp_df_univariate_ad[col] = tmp_df_univariate_ad[col].fillna(method='ffill').fillna(method='bfill')\n",
        "                    df_univariate_ad = pd.concat([df_univariate_ad, tmp_df_univariate_ad[col]], axis=1)\n",
        "                else:\n",
        "                    df_univariate_ad = pd.concat([df_univariate_ad, df_to_univariate_ad[col]], axis=1)\n",
        "            return df_univariate_ad\n",
        "        elif clipping:\n",
        "            return df_clip\n",
        "        else:\n",
        "            print('Caution: Data did not get smoothed.')\n",
        "            return df\n",
        "        \n",
        "    else:\n",
        "        df_gb = df.groupby(['Id', 'Key'])\n",
        "        tmp_initial_twins = df_gb['value'].apply(lambda x: list(x)[0]).reset_index()\n",
        "        tmp_initial_twins_dic = dict(zip(list(zip(tmp_initial_twins['Id'], tmp_initial_twins['Key'])), \n",
        "                                         tmp_initial_twins['value']))\n",
        "\n",
        "        num_df, cat_df = pd.DataFrame(), pd.DataFrame()\n",
        "        for k, v in tmp_initial_twins_dic.items():\n",
        "            try:\n",
        "                float(v)\n",
        "                sub_num_df = df_gb.get_group(k).sort_values('timestamp').reset_index(drop=True)\n",
        "                sub_num_df['value'] = sub_num_df['value'].astype('float')\n",
        "                if clipping:\n",
        "                    mu, std = sub_num_df['value'].mean(), \\\n",
        "                              sub_num_df['value'].std()\n",
        "                    sub_num_df['value'] = sub_num_df['value'].clip(mu-3*std, mu+3*std)\n",
        "                    \n",
        "                if univariate_ad:\n",
        "                    iso_forest = IsolationForest(n_estimators=125)\n",
        "                    tmp_df_univariate_ad = pd.DataFrame(sub_num_df.copy())\n",
        "                    tmp_df_univariate_ad['predictions'] = iso_forest.fit_predict(sub_num_df['value'].to_numpy().reshape(-1,1))\n",
        "                    tmp_df_univariate_ad.loc[tmp_df_univariate_ad['predictions']==-1, 'value'] = np.nan\n",
        "                    tmp_df_univariate_ad['value'] = tmp_df_univariate_ad['value'].fillna(method='ffill').fillna(method='bfill')\n",
        "                    num_df = pd.concat([num_df, tmp_df_univariate_ad[['Id', 'ModelId', 'Key', 'timestamp', 'value']]])\n",
        "                else:\n",
        "                    num_df = pd.concat([num_df, sub_num_df])\n",
        "            except:\n",
        "                cat_df = pd.concat([cat_df, df_gb.get_group(k)])\n",
        "        \n",
        "        if not clipping and not univariate_ad:\n",
        "            print('Caution: Data did not get smoothed.')\n",
        "        df_smooth = pd.concat([num_df, cat_df]).sort_values('timestamp').reset_index(drop=True)\n",
        "        return df_smooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(purpose=None, \\\n",
        "               raw_df=None, \\\n",
        "               invalid_values=['None', 'NaN', 'NA', 'nan', '', ' ', -1], \\\n",
        "               resampling_rate='1min', \\\n",
        "               num_agg_fc='mean', \\\n",
        "               cat_agg_fc='mode', \\\n",
        "               missing_tolerance=1.0, \\\n",
        "               limit=None, \\\n",
        "               num_range_dic_train=None, \\\n",
        "               num_uniqueKeys_train=None, \\\n",
        "               cat_uniqueKeys_values_dic_train=None, \\\n",
        "               values_to_fill_for_inference=None) -> tuple or pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Implement data pre-processing for raw data from ADT-ADX cross query in training pipeline, according to the guidelines indicated in PR FAQ.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    purpose : indicate the purpose of data pre-processing, one of 'training' or 'inference',\n",
        "        str\n",
        "    raw_df : raw training or inference data from ADX, with columns 'Id', 'ModelId', 'Key', 'Timestamp' and 'Value',\n",
        "        Spark DataFrame\n",
        "    invalid_values : list of entries that considered as invalid,\n",
        "        list, default=['None', 'NaN', 'NA', 'nan', '', ' ', -1]\n",
        "    resampling_rate : resampling rate of timestamp,\n",
        "        str, default='1min'\n",
        "    num_agg_fc : numerical variables aggregation function when resampling,\n",
        "        str or function, default='mean'\n",
        "    cat_agg_fc : categorical variables aggregation function when resampling,\n",
        "        str or function, default='mode'\n",
        "    missing_tolerance : tolerance of missing ratio for each variable, only columns with missing ratio not exceeding this threshold will be kept,\n",
        "        float, default=1.0\n",
        "    limit : max number of consecutive missings to fill,\n",
        "        int\n",
        "    num_range_dic_train, num_uniqueKeys_train, cat_uniqueKeys_values_dic_train, values_to_fill_for_inference : params only passed to inference which obtained from training,\n",
        "        for details see Return below.\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    ret_df : pre-processed training or inference data,\n",
        "        Spark DataFrame\n",
        "    num_range_dic_train : dict of all numerical features' min and max summarized from the training dataset,\n",
        "        dict (e.g. {'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ia': {'min': 0.0, 'max': 100},\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ib': {'min': 0.0, 'max': 200},\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ic': {'min': 0.0, 'max': 300})\n",
        "    num_uniqueKeys : list of numerical unique keys seen in the training dataset, each unique keys is formatted as 'ModelId_Id_Key',\n",
        "        list of str (e.g. ['dtmi:syntheticfactory:feedmachine;1_C_Amps_Ia', \n",
        "                           'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ib', \n",
        "                           'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ic'])\n",
        "    cat_uniqueKeys_values_dic : dict of all values seen in the training dataset for each categorical unique key,\n",
        "        dict (e.g. {'dtmi:syntheticfactory:sourcemachine;1_A_PowerLevel': ['High', 'Low', 'Mid'],\n",
        "                    'dtmi:syntheticfactory:sourcemachine;1_B_PowerLevel': ['High', 'Low', 'Mid']}})\n",
        "    values_to_fill_for_inference : dict of default value to fill in during inference for each unique key,\n",
        "        dict (e.g. {'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ia': 0.0,\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ib': 0.0,\n",
        "                    'dtmi:syntheticfactory:feedmachine;1_C_Amps_Ic': 0.0})\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Example input data for data pre-processing:\n",
        "    index   Id                   ModelId                      Key                  Timestamp                      Value\n",
        "    0       E     dtmi:syntheticfactory:feedmachine;1       Amps_Ic        2020-12-31 23:59:59.028164              0.0\n",
        "    1       J     dtmi:syntheticfactory:feedmachine;1       Amps_Ib        2020-12-31 23:59:59.273131     0.0059823441449033355\n",
        "    2       A     dtmi:syntheticfactory:sourcemachine;1     Amps_Ia        2020-12-31 23:59:59.285524     0.002924511047766594\n",
        "    3       A     dtmi:syntheticfactory:sourcemachine;1    PowerLevel      2021-01-01 00:00:00.840780              Mid\n",
        "    4       B     dtmi:syntheticfactory:sourcemachine;1    PowerLevel      2021-01-01 00:00:01.250716              Low\n",
        "    ...    ...                     ...                        ...                     ...                          ...\n",
        "\n",
        "    Example output of pre-processed data:\n",
        "    index       timestamp        dtmi:syntheticfactory:sourcemachine;1_A_Amps_Ia    dtmi:syntheticfactory:sourcemachine;1_A_PowerLevel_High     ...\n",
        "    0      2021-01-01 00:00:00                          0                                                      0                                ...\n",
        "    1      2021-01-01 00:10:00                          0                                                      1                                ...\n",
        "    2      2021-01-01 00:20:00                       0.000114                                                  1                                ...\n",
        "    3      2021-01-01 00:30:00                       0.000097                                                  0                                ...\n",
        "    4      2021-01-01 00:40:00                          0                                                      0                                ...\n",
        "    \"\"\"\n",
        "    ## Step 1. Query & Basic Data Quality Checks\n",
        "    # Step 1.1. Convert Spark to Pandas df, reformat the raw data with columns: 'timestamp', 'UniqueKey', 'value'\n",
        "    print('Data Shape before pre-processing:', (raw_df.count(), len(raw_df.columns)))\n",
        "    if (raw_df.count() == 0):\n",
        "        print('Empty Dataset to pre-process.')\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        raw_df = raw_df.drop('TimeStamp')\n",
        "    except:\n",
        "        pass\n",
        "    raw_df = raw_df.withColumnRenamed('SourceTimeStamp', 'TimeStamp')\n",
        "\n",
        "    # Format timestamp to MVAD accepted format\n",
        "    raw_df = raw_df.withColumn(\n",
        "        'TimeStamp', \n",
        "        F.date_format(F.col('TimeStamp'), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
        "        )\n",
        "    \n",
        "    # Change non alphanumeric or _ characters to _ for ModelId and Id columns\n",
        "    raw_df = raw_df.withColumn(\"ModelId\", F.regexp_replace(F.col(\"ModelId\"), \"[^a-zA-Z0-9_]\", \"_\"))\n",
        "    raw_df = raw_df.withColumn(\"Id\", F.regexp_replace(F.col(\"Id\"), \"[^a-zA-Z0-9_]\", \"_\"))\n",
        "\n",
        "    # Create UniqueKey column to identify unique key per ModelId, Id, Key combinations\n",
        "    raw_df = raw_df.withColumn('UniqueKey', \n",
        "                    F.concat(F.col('ModelId'), F.lit('_'), F.col('Id'), F.lit('_'), F.col('Key')))\n",
        "\n",
        "    # Select needed columns and make column names lowercase\n",
        "    raw_df = raw_df.select(\"TimeStamp\", \"UniqueKey\", \"Value\").withColumnRenamed(\"TimeStamp\", \"timestamp\").withColumnRenamed(\"Value\", \"value\")\n",
        "\n",
        "    # Convert Spark to Pandas df\n",
        "    raw_df = raw_df.toPandas()\n",
        "    \n",
        "    raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'], infer_datetime_format=True) \n",
        "    raw_df = raw_df[['timestamp', 'UniqueKey', 'value']] \\\n",
        "             .sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "    # Step 1.2. Drop duplicate rows in raw historized dataset\n",
        "    df = raw_df.drop_duplicates()\n",
        "    # Step 1.3. Data validity checks\n",
        "    df = df[~df['value'].isin(invalid_values)].reset_index(drop=True)\n",
        "\n",
        "    ## Step 2. Resampling (timestamp alignment) & Pivoting\n",
        "    # Step 2.1. Timestamp binning\n",
        "    df['timestamp'] = df['timestamp'].dt.round(resampling_rate)\n",
        "    # Step 2.2. Table pivoting\n",
        "    tmp_initial_twins = df.groupby('UniqueKey')['value'].apply(lambda x: list(x)[0]).reset_index()\n",
        "    tmp_initial_twins_dic = dict(zip(tmp_initial_twins['UniqueKey'], tmp_initial_twins['value']))\n",
        "    if purpose=='training':\n",
        "        num_uniqueKeys, cat_uniqueKeys = [], []\n",
        "        for k, v in tmp_initial_twins_dic.items():\n",
        "            try:\n",
        "                float(v)\n",
        "                num_uniqueKeys.append(k)\n",
        "            except:\n",
        "                cat_uniqueKeys.append(k)\n",
        "        num_uniqueKeys, cat_uniqueKeys = sorted(num_uniqueKeys), \\\n",
        "                                         sorted(cat_uniqueKeys)\n",
        "    else:\n",
        "        num_uniqueKeys_inf, cat_uniqueKeys_inf = [], []\n",
        "        for k, v in tmp_initial_twins_dic.items():\n",
        "            try:\n",
        "                float(v)\n",
        "                num_uniqueKeys_inf.append(k)\n",
        "            except:\n",
        "                cat_uniqueKeys_inf.append(k)\n",
        "        num_uniqueKeys_inf, cat_uniqueKeys_inf = sorted(num_uniqueKeys_inf), \\\n",
        "                                                 sorted(cat_uniqueKeys_inf)\n",
        "        num_uniqueKeys_unknown, num_uniqueKeys_missing = [col for col in num_uniqueKeys_inf if col not in num_uniqueKeys_train], \\\n",
        "                                                        [col for col in num_uniqueKeys_train if col not in num_uniqueKeys_inf]\n",
        "        cat_uniqueKeys_train = sorted(list(cat_uniqueKeys_values_dic_train.keys()))\n",
        "        cat_uniqueKeys_unknown, cat_uniqueKeys_missing = [col for col in cat_uniqueKeys_inf if col not in cat_uniqueKeys_train], \\\n",
        "                                                        [col for col in cat_uniqueKeys_train if col not in cat_uniqueKeys_inf]\n",
        "        num_uniqueKeys, cat_uniqueKeys = [col for col in num_uniqueKeys_inf if col in num_uniqueKeys_train], \\\n",
        "                                         [col for col in cat_uniqueKeys_inf if col in cat_uniqueKeys_train]\n",
        "\n",
        "    num_df, cat_df = df[df['UniqueKey'].isin(num_uniqueKeys)], \\\n",
        "                     df[df['UniqueKey'].isin(cat_uniqueKeys)]\n",
        "    num_df['value'] = num_df['value'].astype(float)\n",
        "\n",
        "    num_df_after_groupby = num_df.groupby(['timestamp', 'UniqueKey'])['value'].mean().reset_index() if num_agg_fc=='mean' \\\n",
        "                           else num_df.groupby(['timestamp', 'UniqueKey'])['value'].apply(num_agg_fc).reset_index()\n",
        "    cat_df_after_groupby = cat_df.groupby(['timestamp', 'UniqueKey'])['value'].agg(lambda x: pd.Series.mode(x)[0]).reset_index() if cat_agg_fc=='mode' \\\n",
        "                           else cat_df.groupby(['timestamp', 'UniqueKey'])['value'].apply(cat_agg_fc).reset_index()\n",
        "    df_after_groupby = pd.concat([num_df_after_groupby, cat_df_after_groupby])\n",
        "    df_after_groupby = df_after_groupby.sort_values('timestamp').reset_index(drop=True)\n",
        "    df_pivot = df_after_groupby.pivot(index='timestamp', columns='UniqueKey', values='value')\n",
        "    df_pivot.columns.name = None\n",
        "    df_pivot = df_pivot.sort_index().reset_index()\n",
        "\n",
        "    if purpose=='inference':\n",
        "        print(f'Inference - Unknown Numerical Keys and Dropped: {num_uniqueKeys_unknown}')\n",
        "        print(f'Inference - Unknown Categorical Keys and Dropped: {cat_uniqueKeys_unknown}')\n",
        "        print(f'Inference - Missing Numerical Keys: {num_uniqueKeys_missing}, Filled with {[values_to_fill_for_inference[col] for col in num_uniqueKeys_missing]}')\n",
        "        print(f'Inference - Missing Categorical Keys: {cat_uniqueKeys_missing}, Filled with {[values_to_fill_for_inference[col] for col in cat_uniqueKeys_missing]}')\n",
        "        for col in num_uniqueKeys_missing:\n",
        "            df_pivot[col] = values_to_fill_for_inference[col]\n",
        "            df_pivot[col] = df_pivot[col].astype(float)\n",
        "        for col in cat_uniqueKeys_missing:\n",
        "            df_pivot[col] = values_to_fill_for_inference[col]\n",
        "        df_pivot = df_pivot[sorted(df_pivot.columns)]\n",
        "        \n",
        "        cat_uniqueKeys_values_dic_inf = {}\n",
        "        cat_uniqueKeys_values_unknown_dic, cat_uniqueKeys_values_missing_dic = {}, {}\n",
        "        for col in cat_uniqueKeys_train:\n",
        "            cat_uniqueKeys_values_dic_inf[col] = sorted(df_pivot[col][df_pivot[col].notnull()].unique())\n",
        "            cat_uniqueKeys_values_unknown_dic[col] = [v for v in cat_uniqueKeys_values_dic_inf[col] if v not in cat_uniqueKeys_values_dic_train[col]]\n",
        "            cat_uniqueKeys_values_missing_dic[col] = [v for v in cat_uniqueKeys_values_dic_train[col] if v not in cat_uniqueKeys_values_dic_inf[col]]\n",
        "            if cat_uniqueKeys_values_unknown_dic[col]!=[]:\n",
        "                print(f\"\\nInference - Unknown Categorical Values for '{col}' and Ignored: {cat_uniqueKeys_values_unknown_dic[col]}\")\n",
        "                df_pivot[col] = df_pivot[col].replace(cat_uniqueKeys_values_unknown_dic[col], np.nan)\n",
        "\n",
        "    # Step 2.3. Timestamp standardization\n",
        "    missing_timestamp_df = pd.DataFrame(columns=df_pivot.columns)\n",
        "    if df_pivot.shape[0]>1:   \n",
        "        full_time_range = pd.date_range(df_pivot['timestamp'].iloc[0], \\\n",
        "                                        df_pivot['timestamp'].iloc[-1], \\\n",
        "                                        freq=resampling_rate)\n",
        "        full_time_range.freq=None\n",
        "        missing_timestamp_df['timestamp'] = [timestamp for timestamp in full_time_range \\\n",
        "                                            if timestamp not in df_pivot['timestamp'].values]\n",
        "        df_pivot = pd.concat([df_pivot, missing_timestamp_df])\n",
        "    df_pivot = df_pivot.sort_values('timestamp').set_index('timestamp')\n",
        "    # Step 2.4. Calculate missing ratios for each feature\n",
        "    if purpose=='training':\n",
        "        missing_ratios = (df_pivot.isnull().sum()/df_pivot.shape[0]).sort_values(ascending=False)\n",
        "        df_pivot = df_pivot[missing_ratios[missing_ratios<=missing_tolerance].index]\n",
        "        df_pivot = df_pivot[sorted(df_pivot.columns)]\n",
        "\n",
        "        num_uniqueKeys, cat_uniqueKeys = sorted([col for col in num_uniqueKeys if col in df_pivot.columns]), \\\n",
        "                                         sorted([col for col in cat_uniqueKeys if col in df_pivot.columns])\n",
        "        values_to_fill_for_inference = {}\n",
        "        for col in num_uniqueKeys:\n",
        "            values_to_fill_for_inference[col] = df_pivot[col].median()\n",
        "        for col in cat_uniqueKeys:\n",
        "            values_to_fill_for_inference[col] = df_pivot[col].mode()[0]\n",
        "        \n",
        "        cat_uniqueKeys_values_dic = {}\n",
        "        for cat_uniqueKey in cat_uniqueKeys:\n",
        "            cat_uniqueKeys_values_dic[cat_uniqueKey] = sorted(df_pivot[cat_uniqueKey][df_pivot[cat_uniqueKey].notnull()].unique())\n",
        "\n",
        "    ## Step 3. Data-preprocessing on pivoted and standardized table\n",
        "    # Step 3.1. Handling missing NaN values\n",
        "    for col in df_pivot:\n",
        "        if col in num_uniqueKeys:\n",
        "            df_pivot[col] = df_pivot[col].astype(float).interpolate(method='linear', limit=limit)\n",
        "        df_pivot[col] = df_pivot[col].fillna(method='ffill', limit=limit).fillna(method='bfill', limit=limit)\n",
        "    assert df_pivot.isnull().sum().sum()==0\n",
        "    # Step 3.2. Normalization & Encoding\n",
        "    num_df_pivot, cat_df_pivot = df_pivot.select_dtypes(include='float'), \\\n",
        "                                 df_pivot.select_dtypes(include='object')\n",
        "\n",
        "    if purpose=='training':\n",
        "        num_range_dic_train, num_min_dic, num_max_dic = dict(), \\\n",
        "                                                  dict(num_df_pivot.min()), \\\n",
        "                                                  dict(num_df_pivot.max())\n",
        "        for k in num_min_dic:\n",
        "            num_range_dic_train[k] = dict()\n",
        "            num_range_dic_train[k]['min'], num_range_dic_train[k]['max'] = num_min_dic[k], \\\n",
        "                                                               num_max_dic[k]\n",
        "    for col in num_df_pivot.columns:\n",
        "        if num_range_dic_train[col]['max']==num_range_dic_train[col]['min']:\n",
        "            num_df_pivot[col] = num_df_pivot[col]-num_range_dic_train[col]['min']\n",
        "        else:\n",
        "            num_df_pivot[col] = (num_df_pivot[col]-num_range_dic_train[col]['min'])/(num_range_dic_train[col]['max']-num_range_dic_train[col]['min']) \n",
        "    num_df_pivot = num_df_pivot.reset_index()\n",
        "    \n",
        "    if not cat_df_pivot.empty:\n",
        "        cat_df_pivot = pd.get_dummies(cat_df_pivot)\n",
        "    else:\n",
        "        cat_df_pivot = pd.DataFrame(index=cat_df_pivot.index)\n",
        "    cat_df_pivot = cat_df_pivot.reset_index()\n",
        "    if purpose=='inference':\n",
        "        for col in cat_uniqueKeys_train:\n",
        "            if cat_uniqueKeys_values_missing_dic[col]!=[]:\n",
        "                print(f\"Inference - Missing Categorical Values for '{col}', Completed with: {cat_uniqueKeys_values_missing_dic[col]}\")\n",
        "                for missing_value in cat_uniqueKeys_values_missing_dic[col]:\n",
        "                    new_dummy_col_name = col + '_' + missing_value\n",
        "                    cat_df_pivot[new_dummy_col_name] = 0\n",
        "    ret_df = num_df_pivot.merge(cat_df_pivot, on='timestamp', how='inner')\n",
        "    ret_df = ret_df.sort_values('timestamp').reset_index(drop=True)\n",
        "    ret_df = ret_df[sorted(ret_df.columns)]\n",
        "    print(f'Data Shape after pre-processing: {ret_df.shape}')\n",
        "    print(ret_df.head())\n",
        "\n",
        "    ret_df['timestamp'] = ret_df['timestamp'].apply(lambda x: x.isoformat())\n",
        "\n",
        "    if purpose=='training':\n",
        "        return ret_df, num_range_dic_train, num_uniqueKeys, cat_uniqueKeys_values_dic, values_to_fill_for_inference\n",
        "    else:\n",
        "        return ret_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    df = query_adt_data(kusto_linked_service, kusto_database, new_query)\n",
        "    df = (df.withColumnRenamed(adx_mapping_Id, 'Id')\n",
        "            .withColumnRenamed(adx_mapping_key, 'Key')\n",
        "            .withColumnRenamed(adx_mapping_value, 'Value')\n",
        "            .withColumnRenamed(adx_mapping_sourcetime, 'SourceTimestamp'))\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Unable to query data from ADX. Full error log: \" + str(e))\n",
        "\n",
        "if (df.count() == 0):\n",
        "    mssparkutils.notebook.exit(\"Failure;No Data Queried from ADX\")\n",
        "\n",
        "data_preprocessing_train_kwargs = {'purpose': 'training', \\\n",
        "                                   'raw_df': df, \\\n",
        "                                   'resampling_rate': str(resampling_rate) + 'min', \\\n",
        "                                   'num_agg_fc': 'mean', \\\n",
        "                                   'cat_agg_fc': 'mode', \\\n",
        "                                   'missing_tolerance': 1.0}\n",
        "\n",
        "try:\n",
        "    preprocessed_df_train, num_range_dic_train, num_uniqueKeys_train, cat_uniqueKeys_values_dic_train, values_to_fill_for_inference = preprocess(**data_preprocessing_train_kwargs)\n",
        "    if smoothing:\n",
        "        data_smoothing_kwargs = {'df': preprocessed_df_train, \\\n",
        "                                 'preprocessed': True, \\\n",
        "                                 'clipping': True, \\\n",
        "                                 'univariate_ad': False}\n",
        "        preprocessed_df_train = smoothing(**data_smoothing_kwargs)\n",
        "\n",
        "    # Convert dataframe to Spark\n",
        "    preprocessed_df_train = spark.createDataFrame(preprocessed_df_train)\n",
        "    preprocessed_df_train = preprocessed_df_train.withColumn('timestamp', F.concat_ws(\"\",F.col(\"timestamp\"),F.lit(\"Z\")))\n",
        "    \n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Data processing failure. Full error log: \" + str(e))\n",
        "\n",
        "if (preprocessed_df_train.count() == 0):\n",
        "    mssparkutils.notebook.exit(\"Failure;No Data After Data Processing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "important_info = {\n",
        "    \"resampling_rate\": str(resampling_rate) + \"min\",\n",
        "    \"num_agg_fc\": \"mean\",\n",
        "    \"cat_agg_fc\": \"mode\",\n",
        "    \"num_range_dic_train\": num_range_dic_train,\n",
        "    \"num_uniqueKeys_train\": num_uniqueKeys_train,\n",
        "    \"cat_uniqueKeys_values_dic_train\": cat_uniqueKeys_values_dic_train,\n",
        "    \"values_to_fill_for_inference\": values_to_fill_for_inference\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MVAD Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "anomalyKey = token_library.getSecret(kv_name, mvad_kv_secret_name, kv_linked_service)\n",
        "connectionString = token_library.getSecret(kv_name, adls_kv_connection_string_name, kv_linked_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Specify information about your data.\n",
        "timestampColumn = \"timestamp\" \n",
        "# To Fix\n",
        "# inputColumns = num_uniqueKeys_train + list(cat_uniqueKeys_values_dic_train.keys())\n",
        "inputColumns = preprocessed_df_train.columns\n",
        "inputColumns.remove(timestampColumn)\n",
        "\n",
        "#Specify the container you created in Storage account. \n",
        "containerName = adls_container\n",
        "\n",
        "#Set a folder name in Storage account to store the intermediate data. \n",
        "intermediateSaveDir = \"mvad_temporary_files\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    fitMultivariateAnomaly = (FitMultivariateAnomaly()\n",
        "        .setSubscriptionKey(anomalyKey)\n",
        "        .setLocation(mvad_region)\n",
        "        .setOutputCol(\"result\")\n",
        "        .setStartTime(start_time)\n",
        "        .setEndTime(end_time)\n",
        "        .setContainerName(containerName)\n",
        "        .setIntermediateSaveDir(intermediateSaveDir)\n",
        "        .setTimestampCol(timestampColumn)\n",
        "        .setInputCols(inputColumns)\n",
        "        .setSlidingWindow(sliding_window)\n",
        "        .setConnectionString(connectionString)\n",
        "        .setMaxPollingRetries(1))\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Unable to create FitMultivariateAnomaly object. Full error log: \" + str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    model = fitMultivariateAnomaly.fit(preprocessed_df_train)\n",
        "    mvad_model_id = model.getModelId()\n",
        "except Exception as e:\n",
        "    mssparkutils.notebook.exit(\"Failure;Training Failure. Full error log: \" + str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mssparkutils.notebook.exit(\"Success;\" + str(important_info) + \";\" + mvad_model_id)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
