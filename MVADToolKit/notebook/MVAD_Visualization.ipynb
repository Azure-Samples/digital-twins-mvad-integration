{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook aims to provide users convenient visualization tools after Anomaly Detection, and inform how the time-series go and if the MVAD works well. \n",
        "Visualization takes place within Synapse, are made interactive and wrapped up in one-piece so users do not have to switch to ADX or make extra efforts on ADX queries and plots. \n",
        "\n",
        "Specifically, the notebook connects the ADX resulting table and visualize the following for user insights:\n",
        "\n",
        "- Check time-series for pattern and surges and dips.\n",
        "- Highlight where the anomalies are detected and show whether the decision was correct in distinct colored regions with ground truth.\n",
        "- Show severity- how confident the algorithm was to announce anomalies, so customers can adjust this important parameter to catch anomalies as much as possible with minimum noises.\n",
        "- For each triggered anomaly indicate the top telemetry contributors as potential root causes.\n",
        "- Concrete numbers of popular metrics for classification as performance assessment, namely, confusion matrix, f1, precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Section 1. Import modules and define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "from datetime import datetime as dt\n",
        "import pytz\n",
        "import ast\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def query_adx_data(adx_linked_service, adx_database, query):\n",
        "    \"\"\" Query ADT data and return the data as Spark dataframe\n",
        "    :param adx_linked_service: name of the ADX (historized data store) linked service registered in Synapse workspace\n",
        "    :type: string\n",
        "\n",
        "    :param adx_database: ADX database name containing historized ADX data\n",
        "    :type: string\n",
        "\n",
        "    :param query: ADT-ADX joint query\n",
        "    :type: string\n",
        "\n",
        "    :return: dataframe containing queried data\n",
        "    :type: Spark dataframe\n",
        "    \"\"\"\n",
        "    df  = spark.read \\\n",
        "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "        .option(\"spark.synapse.linkedService\", adx_linked_service) \\\n",
        "        .option(\"kustoDatabase\", adx_database) \\\n",
        "        .option(\"kustoQuery\", query) \\\n",
        "        .option(\"authType\", \"LS\") \\\n",
        "        .load()\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_mvad_result_from_adx(\n",
        "    cols_to_include=None, time_window=None,\n",
        "    adx_linked_service=None, adx_database=None, adx_table=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch data with MVAD results from ADX.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    cols_to_include : columns to include for plots,\n",
        "        list of str (e.g. ['dtmi_syntheticfactory_sourcemachine2_1_A_Amps_Ia', \n",
        "                           'dtmi_syntheticfactory_sourcemachine2_1_A_Amps_Ib',\n",
        "                           'dtmi_syntheticfactory_sourcemachine2_1_A_Amps_Ic'])\n",
        "    time_window : indicate whether to save the topology json file,\n",
        "        list of str (e.g. '2022-04-19 04:26:00', '2022-04-19 09:00:00'])\n",
        "    adx_linked_service : name of ADX linkedService,\n",
        "        str\n",
        "    adx_database : name of ADX Database,\n",
        "        str\n",
        "    adx_table : name of ADX MVAD resulting Table,\n",
        "        str\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    df : MVAD inference results with specified columns in certain time window,\n",
        "        pd.DataFrame\n",
        "    \"\"\"\n",
        "    adx_query = f\"{adx_table}\" \n",
        "    if cols_to_include is not None:\n",
        "        adx_query = f\"{adx_table} | project timestamp, result, isAnomaly\"\n",
        "        for col in cols_to_include:\n",
        "            adx_query += f\", {col}\"\n",
        "    if time_window is not None:\n",
        "        start_time = pytz.utc.localize(dt.strptime(time_window[0], '%Y-%m-%d %H:%M:%S'))\n",
        "        end_time = pytz.utc.localize(dt.strptime(time_window[1], '%Y-%m-%d %H:%M:%S'))\n",
        "        adx_query += f\" | where todatetime(timestamp)>=datetime({start_time}) and todatetime(timestamp)<=datetime({end_time})\"\n",
        "        \n",
        "    df = query_adx_data(adx_linked_service, adx_database, adx_query)\n",
        "    \n",
        "    df = df.toPandas()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True).drop('isAnomaly', axis=1)\n",
        "    df['result'] = df['result'].str.replace('false', 'False')\n",
        "    df['result'] = df['result'].str.replace('true', 'True')\n",
        "    df['result'] = df['result'].apply(ast.literal_eval)\n",
        "    df = pd.concat([df, pd.json_normalize(df['result'])], axis=1)\n",
        "    df = df.drop('result', axis=1)\n",
        "    \n",
        "    features = cols_to_include if cols_to_include is not None else [col for col in df.columns if col not in ['timestamp', 'isAnomaly', 'score', 'severity', 'contributors']]\n",
        "    empty_contributor_dic = {}\n",
        "    for i in range(len(features)):\n",
        "        empty_contributor_dic['series_'+str(i)]=0\n",
        "\n",
        "    if 'contributors' in df.columns:\n",
        "        df['contributors'] = df['contributors'].apply(lambda x: parse_contributors(x, empty_contributor_dic))\n",
        "        df = pd.concat([df.drop(['contributors'], axis=1), pd.json_normalize(df['contributors'])], axis=1)\n",
        "        df = df[['timestamp', 'isAnomaly', 'score', 'severity'] + features + [f'series_{i}' for i in range(len(features))]] # Hardcoded until MVAD fix the series naming issue\n",
        "    else:\n",
        "        df = df[['timestamp', 'isAnomaly', 'score', 'severity'] + features]\n",
        "        for i in range(len(features)):\n",
        "            df[f'series_{i}'] = 0\n",
        "    return df\n",
        "\n",
        "\n",
        "def parse_contributors(x=None, empty_contributor_dic=None) -> dict:\n",
        "    \"\"\"\n",
        "    Parse the raw contributors json into clean dict format.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : raw contributors json to parse,\n",
        "        list of dict\n",
        "    empty_contributor_dic : for normal situation, return an empty contributor dict with all contributionScore equal zero,\n",
        "        dict\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    parsed_contributors_dic : parsed contributor dict in the format of {Contributor1: contributionScore1, Contributor2: contributionScore2, ...},\n",
        "        dict\n",
        "    \"\"\"    \n",
        "    if x!=x:\n",
        "        return empty_contributor_dic\n",
        "    parsed_contributors_dic = {}\n",
        "    for tmp_dic in x:\n",
        "        parsed_contributors_dic[tmp_dic['variable']] = tmp_dic['contributionScore']\n",
        "    return parsed_contributors_dic\n",
        "\n",
        "\n",
        "def plot_mvad(df=None, cols_to_plot=None, min_severity=None, label=None) -> None:\n",
        "    \"\"\"\n",
        "    Make the following three MVAD results plots:\n",
        "    1. If labels not given, plot the Time-series with Anomalies Detected; \n",
        "       Otherwise if label is provided, plot MVAD performance that filled with colored regions denote TN/FN/FP/TP/Label NA;\n",
        "    2. Severity of Anomalies;\n",
        "    3. Contribution to Anomalies.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : dataframe fetched from ADX to plot,\n",
        "        pd.DataFrame\n",
        "    cols_to_plot : list of columns to include in the plots,\n",
        "        list of str\n",
        "    min_severity : set the minimum severity threshold to define an anomaly,\n",
        "        float\n",
        "    label : label dataframe to evaluate and plot MVAD performance, with at least two columns of 'Timestamp' and 'isAnomaly',\n",
        "        pd.DataFrame, default=None\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    df_plot = df.copy()\n",
        "    tmp_df_plot_anomaly = df_plot[df_plot['isAnomaly']==True]\n",
        "    df_plot['isAnomaly'] = False\n",
        "    \n",
        "    severity_thres_default = tmp_df_plot_anomaly['severity'].min() if tmp_df_plot_anomaly.shape[0]>0 else 1\n",
        "    severity_thres = min_severity or severity_thres_default\n",
        "    df_plot.loc[df_plot['severity']>=severity_thres, 'isAnomaly'] = True\n",
        "    severity_thres = round(severity_thres, 3)\n",
        "\n",
        "    fig1_title = f'Time-series Plot with Anomalies Detected (Magenta) with min_severity={severity_thres}' if label is None \\\n",
        "        else f'MVAD Performance Plot with min_severity={severity_thres}<br>- Green: TN|Yellow: FN|Blue: FP|Red: TP|Grey: Label NA'\n",
        "    fig1 = make_subplots(subplot_titles=[fig1_title])\n",
        "    for feature in cols_to_plot:\n",
        "        fig1.add_trace(go.Scatter(x=df_plot['timestamp'], y=df_plot[feature], name=feature, mode='lines'), row=1, col=1)\n",
        "\n",
        "    ymax = df_plot[cols_to_plot].max().max()\n",
        "    ymin = df_plot[cols_to_plot].min().min()\n",
        "    update_layout_shapes = []\n",
        "\n",
        "    if label is None:\n",
        "        for i in df_plot[df_plot['isAnomaly']==1].index:\n",
        "            if i!=df_plot.shape[0]-1:\n",
        "                update_layout_shapes.append(dict(type='rect',\n",
        "                                                 xref='x',\n",
        "                                                 yref='y',\n",
        "                                                 x0=str(df_plot.iloc[i]['timestamp']),\n",
        "                                                 y0=str(ymin),\n",
        "                                                 x1=str(df_plot.iloc[i+1]['timestamp']),\n",
        "                                                 y1=str(ymax),\n",
        "                                                 fillcolor='magenta',\n",
        "                                                 layer='below',\n",
        "                                                 opacity=1,\n",
        "                                                 line_width=0))\n",
        "        fig1.update_layout(shapes=update_layout_shapes)\n",
        "    else:\n",
        "        label = label.rename(columns={'Timestamp': 'timestamp', 'isAnomaly': 'isAnomaly_label'})\n",
        "        label['timestamp'] = pd.to_datetime(label['timestamp']).dt.tz_localize('utc')\n",
        "        label = label[['timestamp', 'isAnomaly_label']]\n",
        "        \n",
        "        tmp_df_plot = df_plot[['timestamp', 'isAnomaly']].sort_values('timestamp').reset_index(drop=True)\n",
        "        tmp_df_plot = tmp_df_plot.rename(columns={'isAnomaly': 'isAnomaly_mvad'})\n",
        "        tmp_df_plot_merge = tmp_df_plot.merge(label, how='left', on='timestamp')\n",
        "        tmp_df_plot_merge['confusion'] = tmp_df_plot_merge.apply(\n",
        "            lambda x: 'TP' if x['isAnomaly_mvad']==x['isAnomaly_label']==True\n",
        "                else ('FP' if (x['isAnomaly_mvad']== True) and (x['isAnomaly_label']==False)\n",
        "                else ('TN' if x['isAnomaly_mvad']==x['isAnomaly_label']==False \n",
        "                else ('FN' if (x['isAnomaly_mvad']== False) and (x['isAnomaly_label']==True) \n",
        "                else 'NA'))), axis=1)\n",
        "        tmp_df_plot_merge = tmp_df_plot_merge.set_index('timestamp')\n",
        "        \n",
        "\n",
        "        for j in range(tmp_df_plot_merge.shape[0]-1):\n",
        "            confusion = tmp_df_plot_merge.iloc[j]['confusion']\n",
        "            if confusion == 'TN':\n",
        "                fillcolor = 'green'\n",
        "            elif confusion == 'FN':\n",
        "                fillcolor = 'yellow'\n",
        "            elif confusion == 'FP':\n",
        "                fillcolor = 'blue'\n",
        "            elif confusion == 'TP':\n",
        "                fillcolor = 'red'\n",
        "            else:\n",
        "                fillcolor = 'grey'\n",
        "            update_layout_shapes.append(dict(type='rect',\n",
        "                                             xref='x',\n",
        "                                             yref='y',\n",
        "                                             x0=str(tmp_df_plot_merge.index[j]),\n",
        "                                             y0=str(ymin),\n",
        "                                             x1=str(tmp_df_plot_merge.index[j+1]),\n",
        "                                             y1=str(ymax),\n",
        "                                             fillcolor=fillcolor,\n",
        "                                             layer='below',\n",
        "                                             opacity=0.5,\n",
        "                                             line_width=0))\n",
        "        fig1.update_layout(shapes=update_layout_shapes)\n",
        "    \n",
        "    fig1.update_xaxes(title_text='Timestamp')\n",
        "    fig1.update_yaxes(title_text='Value')\n",
        "    fig1.show()\n",
        "        \n",
        "    fig2 = make_subplots(subplot_titles=[f'Severity of Anomalies with min_severity={severity_thres}'])\n",
        "    fig2.add_trace(go.Scatter(x=df_plot['timestamp'], y=df_plot['severity'], \n",
        "                              name='Severity', mode='lines'))\n",
        "    fig2.add_trace(go.Scatter(x=df_plot['timestamp'], y=[severity_thres]*df_plot.shape[0], \n",
        "                              name='min_severity', mode='lines', line=dict(dash='dash')))\n",
        "    fig2.update_xaxes(title_text='Timestamp')\n",
        "    fig2.update_yaxes(title_text='Value')\n",
        "    fig2.show()\n",
        "\n",
        "    fig3 = px.bar(df_plot.fillna(0), \n",
        "                  x='timestamp', y=[f'series_{i}' for i in range(len(cols_to_plot))], \n",
        "                  title='Contribution to Anomalies')\n",
        "    fig3.update_xaxes(title_text='Timestamp')\n",
        "    fig3.update_yaxes(title_text='Contribution Scores')\n",
        "    fig3.update_layout(title_x=0.5)\n",
        "    fig3.show()\n",
        "\n",
        "\n",
        "def evaluate(label=None, df_res=None, \\\n",
        "             floor_bin='15min', severity_thres=None, print_out=True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate MVAD performance in terms of confusion matrix, f1, precision and recall.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    label : label dataframe to evaluate MVAD performance, with at least two columns of 'timestamp' and 'isAnomaly',\n",
        "        pd.DataFrame\n",
        "    df_res : dataframe fetched from ADX to evaluate,\n",
        "        pd.DataFrame\n",
        "    floor_bin : the length of timestamp bins to aggregate AD results, \n",
        "        for example, if original results are made per minute and floor_bin is set to '15min',\n",
        "        then the outcome of 15min-result is True if and only if any single minute of 15min before aggregation give a True, otherwise False,\n",
        "        str, default='15min'\n",
        "    severity_thres : set the minimum severity threshold to define an anomaly, taking the MVAD default if none specified,\n",
        "        float\n",
        "    print_out : indicate whether to print out confusion matrix and other metrics,\n",
        "        bool, default=True\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    confusion_matrix : confusion matrix of TP, FP, TN and FN,\n",
        "        pd.DataFrame\n",
        "    \"\"\"\n",
        "    tmp_df_res = df_res[['timestamp', 'value.is_anomaly', 'value.severity']]\n",
        "    tmp_df_res['timestamp'] = pd.to_datetime(tmp_df_res['timestamp']).dt.tz_localize(None)\n",
        "    tmp_df_res = tmp_df_res.sort_values('timestamp').reset_index(drop=True)\n",
        "    \n",
        "    severity_thres = severity_thres or tmp_df_res[tmp_df_res['value.is_anomaly']]['value.severity'].min()\n",
        "    tmp_df_res.loc[tmp_df_res['value.severity']<severity_thres, 'value.is_anomaly'] = False\n",
        "    \n",
        "    tmp_label = tmp_df_res[['timestamp']].merge(label, how='inner', on='timestamp')\\\n",
        "                          .sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "    tmp_df_res['timestamp_floor'] = tmp_df_res['timestamp'].dt.floor(floor_bin)\n",
        "    df_res_eval = pd.DataFrame(tmp_df_res.groupby('timestamp_floor')['value.is_anomaly'].apply(lambda x: any(list(x))))\n",
        "    tmp_label['timestamp_floor'] = tmp_label['timestamp'].dt.floor(floor_bin)\n",
        "    label_eval = pd.DataFrame(tmp_label.groupby('timestamp_floor')['isAnomaly'].apply(lambda x: 1 in list(x)))\n",
        "\n",
        "    confusion_matrix = pd.crosstab(label_eval['isAnomaly'], df_res_eval['value.is_anomaly'])\n",
        "    if print_out:\n",
        "        print(confusion_matrix)\n",
        "        print(f'f1: {f1_score(label_eval, df_res_eval):.3f}, precision: {precision_score(label_eval, df_res_eval):.3f}, recall: {recall_score(label_eval, df_res_eval):.3f}')\n",
        "    return confusion_matrix\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Section 2. Config specification and fetch AD results from ADX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Specify ADX Config\n",
        "adx_database = \"mvadbbdb\"\n",
        "adx_table = \"<ADX_TABLE>\"\n",
        "adx_linked_service = \"ADT_Data_History\"\n",
        "\n",
        "# Specify columns to plot\n",
        "cols_to_include = ['dtmi_syntheticfactory_sourcemachine_1_A_water_flow', \n",
        "                    'dtmi_syntheticfactory_sourcemachine_1_A_oil_flow', \n",
        "                    'dtmi_syntheticfactory_sourcemachine_1_B_water_flow', \n",
        "                    'dtmi_syntheticfactory_sourcemachine_1_B_oil_flow']\n",
        "\n",
        "# Specify time_window to plot\n",
        "time_window = ['2022-06-01 00:00:00', '2022-07-01 00:00:00']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "scrolled": false
      },
      "source": [
        "# Fetch data with MVAD results from ADX\n",
        "df = get_mvad_result_from_adx(\n",
        "    cols_to_include=cols_to_include, time_window=time_window,\n",
        "    adx_linked_service=adx_linked_service, adx_database=adx_database, adx_table=adx_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Section 3. Make plots with or without labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1. Make Plots W/O Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Take a look at MVAD results and performance with default severity threshold to define an anomaly\n",
        "min_severity = None\n",
        "label = None\n",
        "\n",
        "plot_mvad(df=df, cols_to_plot=cols_to_include, min_severity=min_severity, label=label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# A lower severity threshold will increase the likelihood of true anomalies capture, \n",
        "# with the risk of more false alarms.\n",
        "min_severity = 0.1\n",
        "label = None\n",
        "\n",
        "plot_mvad(df=df, cols_to_plot=cols_to_include, min_severity=min_severity, label=label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# A higher severity threshold will help filter out more false alarms, \n",
        "# with the risk of missing true anomalies.\n",
        "min_severity = 0.9\n",
        "label = None\n",
        "\n",
        "plot_mvad(df=df, cols_to_plot=cols_to_include, min_severity=min_severity, label=label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2. Make Plots W/ Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# # Take a look at MVAD results and performance with default severity threshold to define an anomaly\n",
        "# min_severity = None\n",
        "# label = pd.read_csv(str(Path(os.getcwd()).parent.parent) + '\\anomaly_label.csv')\n",
        "# floor_bin = '1min'\n",
        "# print(f'With Floor_bin = {floor_bin}, Severity_thres = {min_severity}')\n",
        "\n",
        "# _ = evaluate(label, df.rename(columns={'isAnomaly': 'value.is_anomaly', 'severity': 'value.severity'}), floor_bin, min_severity)\n",
        "\n",
        "# plot_mvad(df=df, cols_to_plot=cols_to_include, min_severity=min_severity, label=label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# # A lower severity threshold will increase the likelihood of true anomalies capture, \n",
        "# # with the risk of more false alarms.\n",
        "# min_severity = 0.1\n",
        "# label = pd.read_csv(str(Path(os.getcwd()).parent.parent) + '\\anomaly_label.csv')\n",
        "# floor_bin = '1min'\n",
        "# print(f'With Floor_bin = {floor_bin}, Severity_thres = {min_severity}')\n",
        "\n",
        "# _ = evaluate(label, df.rename(columns={'isAnomaly': 'value.is_anomaly', 'severity': 'value.severity'}), floor_bin, min_severity)\n",
        "\n",
        "# plot_mvad(df=df, cols_to_plot=cols_to_include, min_severity=min_severity, label=label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# # A higher severity threshold will help filter out more false alarms, \n",
        "# # with the risk of missing true anomalies.\n",
        "# min_severity = 0.9\n",
        "# label = pd.read_csv(str(Path(os.getcwd()).parent.parent) + '\\anomaly_label.csv')\n",
        "# floor_bin = '1min'\n",
        "# print(f'With Floor_bin = {floor_bin}, Severity_thres = {min_severity}')\n",
        "\n",
        "# _ = evaluate(label, df.rename(columns={'isAnomaly': 'value.is_anomaly', 'severity': 'value.severity'}), floor_bin, min_severity)\n",
        "\n",
        "# plot_mvad(df=df, cols_to_plot=cols_to_include, min_severity=min_severity, label=label)"
      ]
    }
  ]
}